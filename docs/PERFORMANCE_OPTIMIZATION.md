# ğŸš€ Äá» Xuáº¥t Tá»‘i Æ¯u Hiá»‡u Suáº¥t Há»‡ Thá»‘ng MoneyPrinterTurbo-Extended

## ğŸ“Š PhÃ¢n TÃ­ch Bottleneck Hiá»‡n Táº¡i

### 1. **Sequential Processing** â³ (TÃ¡c Ä‘á»™ng: **CAO**)

**Váº¥n Ä‘á»:**
- Táº¥t cáº£ cÃ¡c bÆ°á»›c xá»­ lÃ½ cháº¡y tuáº§n tá»± 100%
- Script â†’ Terms â†’ Audio â†’ Videos â†’ Subtitle â†’ Final Video
- Thá»i gian tá»•ng = Tá»•ng thá»i gian tá»«ng bÆ°á»›c

**Æ¯á»›c tÃ­nh thá»i gian:**
```
ğŸ”¹ Generate Script (LLM):     5-20 giÃ¢y
ğŸ”¹ Generate Terms (LLM):      3-10 giÃ¢y  
ğŸ”¹ Generate Audio (TTS):      5-30 giÃ¢y
ğŸ”¹ Download Videos:           30-120 giÃ¢y âš ï¸
ğŸ”¹ Generate Subtitle:         10-60 giÃ¢y
ğŸ”¹ Combine Videos:            20-90 giÃ¢y
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
        Tá»”NG:                73-330 giÃ¢y (1.2-5.5 phÃºt)
```

### 2. **Video Download Sequential** ğŸŒ (TÃ¡c Ä‘á»™ng: **Ráº¤T CAO**)

**Váº¥n Ä‘á» trong [material.py](../app/services/material.py#L245-L280):**
```python
# âŒ Download tá»«ng video má»™t (SEQUENTIAL)
for item in valid_video_items:
    saved_video_path = save_video(video_url=item.url, ...)
    # Request â†’ Wait â†’ Download â†’ Save â†’ Repeat
```

**TÃ¡c Ä‘á»™ng:**
- 5-10 video Ã— 5-15 giÃ¢y/video = **25-150 giÃ¢y**
- Network latency + bandwidth khÃ´ng Ä‘Æ°á»£c táº­n dá»¥ng
- Chá»‰ dÃ¹ng 1 connection Ä‘á»“ng thá»i

### 3. **Semantic Similarity Inefficiency** ğŸ¤– (TÃ¡c Ä‘á»™ng: **TRUNG BÃŒNH**)

**Váº¥n Ä‘á» trong [image_similarity.py](../app/services/image_similarity.py#L44-L47):**
```python
# Rate limiting
INFERENCE_DELAY = 0.15  # 150ms delay má»—i inference
MAX_BATCH_SIZE = 10     # Batch nhá»
```

**TÃ¡c Ä‘á»™ng:**
- 20 videos Ã— 10 segments = 200 comparisons
- 200 Ã— 0.15s = **30 giÃ¢y** chá»‰ cho similarity scoring
- CPU-only mode (khÃ´ng dÃ¹ng GPU)

### 4. **Video Processing Overhead** ğŸ¬ (TÃ¡c Ä‘á»™ng: **CAO**)

**Váº¥n Ä‘á» trong [video.py](../app/services/video.py#L148-L280):**
```python
# Táº¡o nhiá»u temp files
for i, selection in enumerate(selected_videos):
    clip_file = f"{output_dir}/temp-semantic-clip-{i+1}.mp4"
    clip.write_videofile(clip_file, ...)  # Write â†’ Read â†’ Repeat
```

**TÃ¡c Ä‘á»™ng:**
- Má»—i clip viáº¿t ra disk rá»“i Ä‘á»c láº¡i
- I/O overhead: **10-30 giÃ¢y**
- Nhiá»u encode/decode cycles

### 5. **Model Loading Overhead** ğŸ§  (TÃ¡c Ä‘á»™ng: **TRUNG BÃŒNH**)

**Váº¥n Ä‘á»:**
- Whisper model load má»—i task: **5-15 giÃ¢y**
- SentenceTransformer load: **3-8 giÃ¢y**
- CLIP model load: **5-10 giÃ¢y**

---

## ğŸ’¡ Giáº£i PhÃ¡p Tá»‘i Æ¯u (Æ¯á»›c tÃ­nh cáº£i thiá»‡n: **40-60%**)

### âœ… 1. Parallel Video Downloads (Cáº£i thiá»‡n: **60-80%** download time)

**Triá»ƒn khai:**
```python
# File: app/services/material.py
from concurrent.futures import ThreadPoolExecutor, as_completed
import threading

# Add to download_videos function
def download_video_parallel(item, material_directory, max_clip_duration):
    """Download single video in parallel"""
    try:
        logger.info(f"downloading video: {item.url}")
        item_search_term = getattr(item, 'search_term', 'unknown')
        saved_video_path = save_video(
            video_url=item.url, 
            save_dir=material_directory, 
            search_term=item_search_term,
            thumbnail_url=item.thumbnail_url, 
            preview_images=item.preview_images
        )
        if saved_video_path:
            return {
                'path': saved_video_path,
                'url': item.url,
                'duration': min(max_clip_duration, item.duration),
                'search_term': item_search_term
            }
    except Exception as e:
        logger.error(f"failed to download video: {item.url} => {str(e)}")
    return None

# Replace sequential loop with parallel downloads
video_paths = []
total_duration = 0.0
downloaded_urls = set()
max_workers = 5  # Concurrent downloads

with ThreadPoolExecutor(max_workers=max_workers) as executor:
    # Submit all download tasks
    future_to_item = {
        executor.submit(download_video_parallel, item, material_directory, max_clip_duration): item 
        for item in valid_video_items
    }
    
    # Process completed downloads as they finish
    for future in as_completed(future_to_item):
        if total_duration > audio_duration:
            logger.info(f"total duration reached, cancelling remaining downloads")
            executor.shutdown(wait=False, cancel_futures=True)
            break
            
        result = future.result()
        if result and result['url'] not in downloaded_urls:
            video_paths.append(result['path'])
            downloaded_urls.add(result['url'])
            total_duration += result['duration']
            logger.info(f"âœ“ downloaded: {result['path']} ({total_duration:.1f}/{audio_duration:.1f}s)")
```

**Lá»£i Ã­ch:**
- `n` video download song song (n=5): **25-150 giÃ¢y â†’ 5-30 giÃ¢y**
- Táº­n dá»¥ng network bandwidth
- Download dá»«ng sá»›m khi Ä‘á»§ duration

---

### âœ… 2. Parallel LLM Calls (Cáº£i thiá»‡n: **40-50%** LLM time)

**Triá»ƒn khai:**
```python
# File: app/services/task.py
from concurrent.futures import ThreadPoolExecutor

def generate_script_and_terms_parallel(task_id, params):
    """Generate script and prepare terms generation in parallel if possible"""
    
    # Generate script first (required)
    video_script = generate_script(task_id, params)
    if not video_script or "Error: " in video_script:
        return None, None
    
    # If video source is not local, generate terms in background
    if params.video_source != "local":
        video_terms = generate_terms(task_id, params, video_script)
        return video_script, video_terms
    
    return video_script, ""

# Usage in start() function:
video_script, video_terms = generate_script_and_terms_parallel(task_id, params)
```

**Lá»£i Ã­ch:**
- Giáº£m latency náº¿u LLM há»— trá»£ multiple concurrent requests
- Pipeline processing: báº¯t Ä‘áº§u audio generation ngay khi cÃ³ script

---

### âœ… 3. Optimize Semantic Video Selection (Cáº£i thiá»‡n: **50-70%**)

**Triá»ƒn khai:**
```python
# File: app/services/image_similarity.py

# TÄƒng batch size vÃ  giáº£m delay
MAX_BATCH_SIZE = 50    # Tá»« 10 â†’ 50
INFERENCE_DELAY = 0.05  # Tá»« 0.15 â†’ 0.05

# Batch encoding embeddings
def compute_similarities_batch(text_segments: List[str], video_metadata: List[Dict]) -> np.ndarray:
    """Compute all similarities in batches"""
    global _clip_model, _clip_processor
    
    # Encode all texts at once
    text_inputs = _clip_processor(text=text_segments, return_tensors="pt", padding=True)
    with torch.no_grad():
        text_embeddings = _clip_model.get_text_features(**text_inputs)
        text_embeddings = text_embeddings / text_embeddings.norm(dim=-1, keepdim=True)
    
    # Encode all images at once (if available)
    all_similarities = []
    for video in video_metadata:
        if video.get('thumbnail_url'):
            # Process images in batch too
            image = download_and_load_image(video['thumbnail_url'])
            image_input = _clip_processor(images=image, return_tensors="pt")
            with torch.no_grad():
                image_embedding = _clip_model.get_image_features(**image_input)
                image_embedding = image_embedding / image_embedding.norm(dim=-1, keepdim=True)
            
            # Compute similarities for all texts at once
            similarities = (text_embeddings @ image_embedding.T).squeeze()
            all_similarities.append(similarities.cpu().numpy())
    
    return np.array(all_similarities)
```

**Lá»£i Ã­ch:**
- **30 giÃ¢y â†’ 5-10 giÃ¢y** cho similarity scoring
- Batch processing táº¥t cáº£ embeddings cÃ¹ng lÃºc
- Ãt inference calls hÆ¡n

---

### âœ… 4. Reduce Video Processing Overhead (Cáº£i thiá»‡n: **30-40%**)

**Triá»ƒn khai:**
```python
# File: app/services/video.py

# Option 1: Keep clips in memory (náº¿u Ä‘á»§ RAM)
def combine_videos_optimized(
    combined_video_path: str,
    video_paths: List[str],
    audio_file: str,
    ...
):
    clips_in_memory = []  # Keep in memory instead of writing to disk
    
    for i, selection in enumerate(selected_videos):
        video_path = selection['video_path']
        clip = VideoFileClip(video_path)
        # Process clip...
        clip = clip.subclipped(start_time, start_time + clip_duration)
        clip = clip.resized(new_size=(video_width, video_height))
        
        # âœ… ADD TO MEMORY LIST instead of writing to disk
        clips_in_memory.append(clip)
        
    # Concatenate all clips at once
    final_clip = concatenate_videoclips(clips_in_memory, method="compose")
    
    # Write once
    final_clip.write_videofile(
        combined_video_path,
        fps=fps,
        codec=video_codec,
        threads=threads,
        logger=None
    )
    
    # Cleanup
    for clip in clips_in_memory:
        close_clip(clip)

# Option 2: Use ffmpeg concat demuxer (fastest)
def concat_with_ffmpeg(clip_files: List[str], output_file: str):
    """Use ffmpeg concat for faster video merging"""
    import subprocess
    
    # Create concat file
    concat_file = output_file + ".txt"
    with open(concat_file, 'w') as f:
        for clip_file in clip_files:
            f.write(f"file '{clip_file}'\n")
    
    # Run ffmpeg concat
    cmd = [
        'ffmpeg', '-f', 'concat', '-safe', '0',
        '-i', concat_file,
        '-c', 'copy',  # Copy without re-encoding
        output_file
    ]
    subprocess.run(cmd, check=True)
    os.remove(concat_file)
```

**Lá»£i Ã­ch:**
- **20-30 giÃ¢y** tiáº¿t kiá»‡m tá»« I/O overhead
- Ãt temp files, Ã­t disk writes

---

### âœ… 5. Pre-load vÃ  Cache Models (Cáº£i thiá»‡n: **100%** cho task thá»© 2+)

**Triá»ƒn khai:**
```python
# File: app/services/model_manager.py (NEW FILE)
from typing import Optional
import threading

class ModelManager:
    """Singleton to manage all AI models"""
    _instance = None
    _lock = threading.Lock()
    
    def __init__(self):
        self.whisper_model = None
        self.sentence_transformer = None
        self.clip_model = None
        self.chatterbox_model = None
    
    @classmethod
    def get_instance(cls):
        if cls._instance is None:
            with cls._lock:
                if cls._instance is None:
                    cls._instance = ModelManager()
        return cls._instance
    
    def get_whisper(self):
        """Lazy load and cache Whisper model"""
        if self.whisper_model is None:
            from faster_whisper import WhisperModel
            from app.config import config
            
            model_size = config.whisper.get("model_size", "large-v3")
            device = config.whisper.get("device", "cpu")
            compute_type = config.whisper.get("compute_type", "int8")
            
            logger.info(f"ğŸ”§ Pre-loading Whisper model: {model_size}")
            self.whisper_model = WhisperModel(
                model_size_or_path=model_size,
                device=device,
                compute_type=compute_type
            )
            logger.success(f"âœ… Whisper model cached")
        
        return self.whisper_model
    
    def get_sentence_transformer(self, model_name: str = "all-mpnet-base-v2"):
        """Lazy load and cache SentenceTransformer"""
        if self.sentence_transformer is None:
            from sentence_transformers import SentenceTransformer
            
            logger.info(f"ğŸ”§ Pre-loading SentenceTransformer: {model_name}")
            self.sentence_transformer = SentenceTransformer(model_name, device='cpu')
            logger.success(f"âœ… SentenceTransformer cached")
        
        return self.sentence_transformer
    
    def preload_all(self):
        """Pre-load all models at startup"""
        logger.info("ğŸš€ Pre-loading all AI models...")
        
        with ThreadPoolExecutor(max_workers=3) as executor:
            futures = [
                executor.submit(self.get_whisper),
                executor.submit(self.get_sentence_transformer),
            ]
            
            for future in as_completed(futures):
                try:
                    future.result()
                except Exception as e:
                    logger.error(f"Failed to pre-load model: {e}")
        
        logger.success("âœ… All models pre-loaded and cached!")

# Usage in main.py or app startup:
from app.services.model_manager import ModelManager

@app.on_event("startup")
async def startup_event():
    # Pre-load models in background thread to not block startup
    import threading
    threading.Thread(target=ModelManager.get_instance().preload_all, daemon=True).start()
```

**Usage in services:**
```python
# In subtitle.py
from app.services.model_manager import ModelManager

def create(audio_file, subtitle_file: str = ""):
    # âœ… Use cached model instead of loading each time
    model = ModelManager.get_instance().get_whisper()
    segments, info = model.transcribe(audio_file, ...)
```

**Lá»£i Ã­ch:**
- **Task Ä‘áº§u tiÃªn:** Load 1 láº§n khi startup (~10-15 giÃ¢y)
- **Task thá»© 2+:** 0 giÃ¢y load time (**100% cáº£i thiá»‡n**)
- Models sáºµn sÃ ng ngay láº­p tá»©c

---

### âœ… 6. Enable GPU Acceleration (Cáº£i thiá»‡n: **200-400%** cho AI tasks)

**Cáº¥u hÃ¬nh:**
```toml
# config.toml
[whisper]
device = "cuda"  # Thay vÃ¬ "cpu"
compute_type = "float16"  # Thay vÃ¬ "int8"

[app]
# Force semantic models to use GPU
semantic_device = "cuda"
```

**Update code:**
```python
# semantic_video.py
device = config.app.get("semantic_device", "cpu")
_model = SentenceTransformer(model_name, device=device)

# image_similarity.py
_force_cpu_only = False  # Allow GPU usage
```

**Lá»£i Ã­ch (vá»›i NVIDIA GPU):**
- Whisper: **10-60 giÃ¢y â†’ 2-15 giÃ¢y** (4-5Ã— faster)
- Semantic similarity: **5-10 giÃ¢y â†’ 1-2 giÃ¢y** (5Ã— faster)
- CLIP inference: **30 giÃ¢y â†’ 6-8 giÃ¢y** (4-5Ã— faster)

---

### âœ… 7. Implement Smart Caching (Cáº£i thiá»‡n: Variable)

**Triá»ƒn khai:**
```python
# File: app/services/cache_manager.py (NEW FILE)
import hashlib
import pickle
from pathlib import Path
from typing import Optional, Any

class CacheManager:
    """Cache for expensive operations"""
    
    def __init__(self, cache_dir: str = "storage/cache"):
        self.cache_dir = Path(cache_dir)
        self.cache_dir.mkdir(parents=True, exist_ok=True)
    
    def get_cache_key(self, *args, **kwargs) -> str:
        """Generate cache key from arguments"""
        key_str = str(args) + str(sorted(kwargs.items()))
        return hashlib.md5(key_str.encode()).hexdigest()
    
    def get(self, key: str) -> Optional[Any]:
        """Get cached value"""
        cache_file = self.cache_dir / f"{key}.pkl"
        if cache_file.exists():
            try:
                with open(cache_file, 'rb') as f:
                    return pickle.load(f)
            except Exception as e:
                logger.warning(f"Cache read failed: {e}")
        return None
    
    def set(self, key: str, value: Any):
        """Set cache value"""
        cache_file = self.cache_dir / f"{key}.pkl"
        try:
            with open(cache_file, 'wb') as f:
                pickle.dump(value, f)
        except Exception as e:
            logger.warning(f"Cache write failed: {e}")

# Usage example:
cache = CacheManager()

def generate_script_with_cache(video_subject: str, language: str) -> str:
    """Generate script with caching"""
    cache_key = cache.get_cache_key(video_subject, language, "script_v1")
    
    # Try to get from cache
    cached_script = cache.get(cache_key)
    if cached_script:
        logger.info(f"âœ“ Using cached script for: {video_subject}")
        return cached_script
    
    # Generate new
    script = llm.generate_script(video_subject=video_subject, language=language)
    
    # Cache it
    cache.set(cache_key, script)
    return script
```

**Cache candidates:**
- LLM responses (script, terms) - **5-30 giÃ¢y** saved
- Video metadata & embeddings - **5-10 giÃ¢y** saved
- Downloaded videos (already implemented)
- Subtitle timing data

---

## ğŸ“ˆ Tá»•ng Káº¿t Cáº£i Thiá»‡n Dá»± Kiáº¿n

| Tá»‘i Æ¯u | Thá»i Gian Tiáº¿t Kiá»‡m | Äá»™ KhÃ³ | Æ¯u TiÃªn |
|---------|---------------------|---------|---------|
| **Parallel Video Downloads** | 20-120 giÃ¢y | Dá»… | ğŸ”´ CAO |
| **Pre-load Models** | 10-25 giÃ¢y (task 2+) | Trung bÃ¬nh | ğŸ”´ CAO |
| **Optimize Semantic Similarity** | 20-25 giÃ¢y | Trung bÃ¬nh | ğŸŸ¡ TB |
| **Reduce Video Processing Overhead** | 10-30 giÃ¢y | KhÃ³ | ğŸŸ¡ TB |
| **GPU Acceleration** | 30-90 giÃ¢y | Dá»… (náº¿u cÃ³ GPU) | ğŸ”´ CAO |
| **Smart Caching** | Variable | Trung bÃ¬nh | ğŸŸ¢ THáº¤P |
| **Parallel LLM Calls** | 3-10 giÃ¢y | Dá»… | ğŸŸ¢ THáº¤P |

### Tá»•ng cáº£i thiá»‡n:
```
âŒ HIá»†N Táº I:  73-330 giÃ¢y (1.2-5.5 phÃºt)
âœ… SAU Tá»I Æ¯U: 30-150 giÃ¢y (0.5-2.5 phÃºt)

ğŸ“Š Cáº¢I THIá»†N: 40-60% nhanh hÆ¡n
   (vá»›i GPU: 50-70% nhanh hÆ¡n)
```

---

## ğŸš€ Káº¿ Hoáº¡ch Triá»ƒn Khai

### Phase 1: Quick Wins (1-2 ngÃ y)
1. âœ… Parallel video downloads
2. âœ… Model pre-loading & caching
3. âœ… TÄƒng batch size cho semantic similarity

### Phase 2: Medium Improvements (3-5 ngÃ y)
4. âœ… Optimize video processing (reduce temp files)
5. âœ… GPU acceleration setup
6. âœ… Parallel LLM calls

### Phase 3: Advanced Features (1 tuáº§n)
7. âœ… Smart caching system
8. âœ… Database for task state (thay vÃ¬ file-based)
9. âœ… Queue management cho multiple concurrent tasks

---

## ğŸ“ Monitoring & Metrics

**ThÃªm performance logging:**
```python
import time
from functools import wraps

def timing_decorator(func_name: str):
    """Decorator to measure function execution time"""
    def decorator(func):
        @wraps(func)
        def wrapper(*args, **kwargs):
            start = time.time()
            result = func(*args, **kwargs)
            duration = time.time() - start
            logger.info(f"â±ï¸  {func_name} took {duration:.2f}s")
            return result
        return wrapper
    return decorator

# Usage:
@timing_decorator("Generate Script")
def generate_script(task_id, params):
    ...

@timing_decorator("Download Videos")  
def download_videos(...):
    ...
```

**Track metrics:**
- Total task duration
- Each step duration
- Bottleneck identification
- Memory usage
- GPU utilization (if available)

---

## âœ… Checklist Triá»ƒn Khai

- [ ] Implement parallel video downloads
- [ ] Create ModelManager singleton
- [ ] Pre-load models at startup
- [ ] Optimize semantic similarity batch processing
- [ ] Reduce video processing temp files
- [ ] Add GPU support configuration
- [ ] Implement CacheManager
- [ ] Add timing decorators
- [ ] Create performance dashboard
- [ ] Load testing vá»›i 10+ concurrent tasks
- [ ] Document new configuration options
- [ ] Update README with performance benchmarks

---

**NgÃ y táº¡o:** 2026-02-08  
**TÃ¡c giáº£:** GitHub Copilot  
**Version:** 1.0
