# ğŸš€ Quick Start: Tá»‘i Æ¯u Hiá»‡u Suáº¥t Ngay Láº­p Tá»©c

## ğŸ¯ Top 3 Cáº£i Thiá»‡n Quan Trá»ng Nháº¥t

### 1ï¸âƒ£ Parallel Video Downloads (60-80% faster) âš¡

**Thá»i gian triá»ƒn khai:** 10 phÃºt  
**Cáº£i thiá»‡n:** 20-120 giÃ¢y â†’ 5-30 giÃ¢y

**BÆ°á»›c 1:** Má»Ÿ file [app/services/material.py](../app/services/material.py)

**BÆ°á»›c 2:** ThÃªm import á»Ÿ Ä‘áº§u file:
```python
from concurrent.futures import ThreadPoolExecutor, as_completed
import time
```

**BÆ°á»›c 3:** Thay tháº¿ code trong function `download_videos()`, tÃ¬m Ä‘oáº¡n:
```python
# âŒ OLD CODE (sequential)
for item in valid_video_items:
    try:
        logger.info(f"downloading video: {item.url}")
        item_search_term = getattr(item, 'search_term', 'unknown')
        saved_video_path = save_video(
            video_url=item.url, save_dir=material_directory, 
            search_term=item_search_term, thumbnail_url=item.thumbnail_url, 
            preview_images=item.preview_images
        )
        # ... rest of code
```

**BÆ°á»›c 4:** Thay báº±ng code má»›i (parallel):
```python
# âœ… NEW CODE (parallel)
def download_single_video(item):
    """Helper function for parallel downloads"""
    try:
        item_search_term = getattr(item, 'search_term', 'unknown')
        saved_video_path = save_video(
            video_url=item.url, save_dir=material_directory,
            search_term=item_search_term, thumbnail_url=item.thumbnail_url,
            preview_images=item.preview_images
        )
        if saved_video_path:
            return {
                'path': saved_video_path,
                'url': item.url,
                'duration': min(max_clip_duration, item.duration),
                'search_term': item_search_term
            }
    except Exception as e:
        logger.error(f"failed to download video: {item.url} => {str(e)}")
    return None

# Parallel download vá»›i ThreadPoolExecutor
max_workers = 5  # Download 5 videos cÃ¹ng lÃºc
with ThreadPoolExecutor(max_workers=max_workers) as executor:
    future_to_item = {
        executor.submit(download_single_video, item): item 
        for item in valid_video_items
    }
    
    for future in as_completed(future_to_item):
        if total_duration > audio_duration:
            logger.info("âœ“ ÄÃ£ Ä‘á»§ video, stopping downloads...")
            break
            
        result = future.result()
        if result and result['url'] not in downloaded_urls:
            video_paths.append(result['path'])
            downloaded_urls.add(result['url'])
            total_duration += result['duration']
            logger.info(f"âœ“ {len(video_paths)} videos ({total_duration:.1f}s/{audio_duration:.1f}s)")
```

**Xem code Ä‘áº§y Ä‘á»§:** [docs/examples/parallel_downloads.py](examples/parallel_downloads.py)

---

### 2ï¸âƒ£ Model Pre-loading & Caching (100% faster cho task 2+) ğŸ§ 

**Thá»i gian triá»ƒn khai:** 20 phÃºt  
**Cáº£i thiá»‡n:** Task 2+ khÃ´ng cáº§n load model láº¡i (tiáº¿t kiá»‡m 10-25 giÃ¢y)

**BÆ°á»›c 1:** Táº¡o file má»›i `app/services/model_manager.py`

Copy toÃ n bá»™ code tá»«: [docs/examples/model_manager.py](examples/model_manager.py)

**BÆ°á»›c 2:** Cáº­p nháº­t [main.py](../main.py):
```python
from app.services.model_manager import setup_model_preloading

if __name__ == "__main__":
    # ğŸš€ Pre-load models trong background
    setup_model_preloading()
    
    logger.info("start server...")
    uvicorn.run(...)
```

**BÆ°á»›c 3:** Cáº­p nháº­t [app/services/subtitle.py](../app/services/subtitle.py):

TÃ¬m:
```python
def create(audio_file, subtitle_file: str = ""):
    global model
    if not model:
        # Old: Load model má»—i láº§n
        model = WhisperModel(...)
```

Thay báº±ng:
```python
def create(audio_file, subtitle_file: str = ""):
    # âœ… New: Use cached model
    from app.services.model_manager import ModelManager
    model = ModelManager.get_instance().get_whisper()
```

**BÆ°á»›c 4:** Cáº­p nháº­t [app/services/semantic_video.py](../app/services/semantic_video.py):

TÃ¬m:
```python
def load_model(model_name: str = "all-mpnet-base-v2"):
    global _model, _model_name
    if _model is None or _model_name != model_name:
        _model = SentenceTransformer(model_name, device='cpu')
```

Thay báº±ng:
```python
def load_model(model_name: str = "all-mpnet-base-v2"):
    # âœ… New: Use cached model
    from app.services.model_manager import ModelManager
    return ModelManager.get_instance().get_sentence_transformer(model_name)
```

---

### 3ï¸âƒ£ Tá»‘i Æ¯u Semantic Similarity (50-70% faster) ğŸ”

**Thá»i gian triá»ƒn khai:** 5 phÃºt  
**Cáº£i thiá»‡n:** 30 giÃ¢y â†’ 5-10 giÃ¢y

**BÆ°á»›c 1:** Má»Ÿ [app/services/image_similarity.py](../app/services/image_similarity.py)

**BÆ°á»›c 2:** TÃ¬m vÃ  thay Ä‘á»•i cÃ¡c constants:
```python
# âŒ OLD (slow)
INFERENCE_DELAY = 0.15
MAX_BATCH_SIZE = 10

# âœ… NEW (fast)
INFERENCE_DELAY = 0.05  # Giáº£m delay tá»« 150ms â†’ 50ms
MAX_BATCH_SIZE = 50     # TÄƒng batch size tá»« 10 â†’ 50
```

**BÆ°á»›c 3:** TÃ¬m dÃ²ng:
```python
_force_cpu_only = True  # Force CPU-only mode
```

Náº¿u báº¡n cÃ³ **NVIDIA GPU**, thay báº±ng:
```python
_force_cpu_only = False  # Allow GPU usage
```

---

## ğŸ“Š Káº¿t Quáº£ Mong Äá»£i

### TrÆ°á»›c khi tá»‘i Æ°u:
```
ğŸ”¹ Generate Script:     10 giÃ¢y
ğŸ”¹ Generate Terms:      5 giÃ¢y
ğŸ”¹ Generate Audio:      15 giÃ¢y
ğŸ”¹ Download Videos:     90 giÃ¢y âš ï¸  (SLOW)
ğŸ”¹ Semantic Match:      30 giÃ¢y âš ï¸  (SLOW)
ğŸ”¹ Generate Subtitle:   20 giÃ¢y (+ 10s load model)
ğŸ”¹ Combine Videos:      40 giÃ¢y
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
   Tá»”NG:               220 giÃ¢y (3.7 phÃºt)
```

### Sau khi tá»‘i Æ°u (3 cáº£i thiá»‡n trÃªn):
```
ğŸ”¹ Generate Script:     10 giÃ¢y
ğŸ”¹ Generate Terms:      5 giÃ¢y
ğŸ”¹ Generate Audio:      15 giÃ¢y
ğŸ”¹ Download Videos:     20 giÃ¢y âœ… (60-80% faster)
ğŸ”¹ Semantic Match:      8 giÃ¢y  âœ… (70% faster)
ğŸ”¹ Generate Subtitle:   20 giÃ¢y (no load time!) âœ…
ğŸ”¹ Combine Videos:      40 giÃ¢y
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
   Tá»”NG:               118 giÃ¢y (2.0 phÃºt)

ğŸš€ Cáº¢I THIá»†N: 46% NHANH HÆ N!
```

---

## ğŸ§ª Test & Verify

### 1. Test parallel downloads:
```bash
# Cháº¡y WebUI hoáº·c API
python main.py

# Táº¡o 1 video vÃ  xem logs
# TÃ¬m dÃ²ng:
# "ğŸš€ Speedup: X.XÃ— faster than sequential"
```

### 2. Test model caching:
```bash
# Task 1: Sáº½ tháº¥y "Loading Whisper model..."
# Task 2: KHÃ”NG tháº¥y "Loading..." ná»¯a - model Ä‘Ã£ cached!
```

### 3. Kiá»ƒm tra memory:
```python
from app.services.model_manager import ModelManager
status = ModelManager.get_instance().get_status()
print(status)
```

---

## ğŸ›ï¸ Cáº¥u HÃ¬nh NÃ¢ng Cao (Optional)

### Config file: [config.toml](../config.example.toml)

```toml
[app]
# Parallel downloads
max_download_workers = 5  # 5-10 recommended (cÃ ng cao cÃ ng nhanh nhÆ°ng tá»‘n RAM)

# Semantic search
enable_semantic_search = true
semantic_device = "cuda"  # or "cpu" (CUDA = 4-5Ã— faster)

# Image similarity
enable_image_similarity = true
image_similarity_model = "clip-vit-base-patch32"
clip_device = "cuda"  # or "cpu"

[whisper]
model_size = "base"  # "tiny/base/small/medium/large-v3"
device = "cuda"      # or "cpu" (CUDA = 5-10Ã— faster)
compute_type = "float16"  # or "int8" cho CPU
```

### Lá»±a chá»n Whisper model size:
- **tiny** - Nhanh nháº¥t, Ä‘á»™ chÃ­nh xÃ¡c tháº¥p (1-2 giÃ¢y)
- **base** - Balance tá»‘t (3-5 giÃ¢y) âœ… Recommended
- **small** - ChÃ­nh xÃ¡c hÆ¡n (5-8 giÃ¢y)
- **medium** - Ráº¥t chÃ­nh xÃ¡c (10-15 giÃ¢y)
- **large-v3** - Tá»‘t nháº¥t, cháº­m nháº¥t (20-60 giÃ¢y)

---

## ğŸ› Troubleshooting

### Lá»—i: "CUDA out of memory"
**Giáº£i phÃ¡p:**
```toml
# config.toml
[whisper]
device = "cpu"  # Fallback vá» CPU

[app]
semantic_device = "cpu"
clip_device = "cpu"
```

### Lá»—i: "Too many open files"
**Giáº£i phÃ¡p:** Giáº£m sá»‘ workers:
```python
max_workers = 3  # Thay vÃ¬ 5-10
```

### Downloads váº«n cháº­m
**NguyÃªn nhÃ¢n:** Network slow hoáº·c proxy issues  
**Giáº£i phÃ¡p:**
```toml
[proxy]
http = ""   # Thá»­ disable proxy
https = ""
```

### Model loading failed
**Giáº£i phÃ¡p:** Download model manually:
```bash
# Whisper
mkdir -p models
cd models
# Download tá»«: https://huggingface.co/Systran/faster-whisper-base

# SentenceTransformer
python -c "from sentence_transformers import SentenceTransformer; SentenceTransformer('all-mpnet-base-v2')"
```

---

## ğŸ“– TÃ i Liá»‡u Äáº§y Äá»§

- ğŸ“„ [Performance Optimization Guide](PERFORMANCE_OPTIMIZATION.md) - Chi tiáº¿t Ä‘áº§y Ä‘á»§
- ğŸ’» [Parallel Downloads Example](examples/parallel_downloads.py) - Code máº«u
- ğŸ§  [Model Manager Example](examples/model_manager.py) - Code máº«u

---

## âœ… Checklist

- [ ] Implement parallel downloads â†’ Test â†’ Verify speedup
- [ ] Add model_manager.py â†’ Update services â†’ Test caching
- [ ] Optimize semantic similarity settings
- [ ] (Optional) Enable GPU acceleration
- [ ] (Optional) Add smart caching layer
- [ ] Measure & compare before/after performance
- [ ] Update config.toml vá»›i settings phÃ¹ há»£p
- [ ] Document changes trong README

---

**ChÃºc báº¡n tá»‘i Æ°u thÃ nh cÃ´ng! ğŸš€**

Náº¿u cÃ³ váº¥n Ä‘á», check logs hoáº·c má»Ÿ issue trÃªn GitHub.
